import math
from functools import partial
from itertools import repeat
import numpy as np
from sklearn.preprocessing import normalize
from sklearn.cluster import KMeans, AgglomerativeClustering
from collections import Counter
import json

import torch
import torch.nn as nn
import torch.nn.functional as F
import collections.abc as container_abcs
# from torch._six import container_abcs
from .vit_pytorch import PatchEmbed, Attention, Mlp, Block, TransReID
from ..make_model import vit_base_patch16_224_TransReID as ori_vit_base_patch16_224_TransReID
from ..make_model import vit_small_patch16_224_TransReID as ori_vit_small_patch16_224_TransReID


# From PyTorch internals
def _ntuple(n):
    def parse(x):
        if isinstance(x, container_abcs.Iterable):
            return x
        return tuple(repeat(x, n))
    return parse

IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)
IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)
to_2tuple = _ntuple(2)

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        print("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def trunc_normal_init(module: nn.Module,
                      mean: float = 0,
                      std: float = 1,
                      a: float = -2,
                      b: float = 2,
                      bias: float = 0) -> None:
    if hasattr(module, 'weight') and module.weight is not None:
        #trunc_normal_(module.weight, mean, std, a, b)  # type: ignore
        _no_grad_trunc_normal_(module.weight, mean, std, a, b)  # type: ignore
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)  # type: ignore

class GeneralizedMeanPooling(nn.Module):
    r"""Applies a 2D power-average adaptive pooling over an input signal composed of several input planes.
    The function computed is: :math:`f(X) = pow(sum(pow(X, p)), 1/p)`
        - At p = infinity, one gets Max Pooling
        - At p = 1, one gets Average Pooling
    The output is of size H x W, for any input size.
    The number of output features is equal to the number of input planes.
    Args:
        output_size: the target output size of the image of the form H x W.
                     Can be a tuple (H, W) or a single H for a square image H x H
                     H and W can be either a ``int``, or ``None`` which means the size will
                     be the same as that of the input.
    """

    def __init__(self, norm=3, output_size=1, eps=1e-6):
        super(GeneralizedMeanPooling, self).__init__()
        assert norm > 0
        self.p = float(norm)
        self.output_size = output_size
        self.eps = eps

    def forward(self, x):
        x = x.clamp(min=self.eps).pow(self.p)
        return F.adaptive_avg_pool1d(x, self.output_size).pow(1. / self.p)

def drop_path(x, drop_prob: float = 0., training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.

    """
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output

class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)

def resize_pos_embed(posemb, posemb_new, hight, width, hw_ratio):
    # Rescale the grid of position embeddings when loading from state_dict. Adapted from
    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224
    ntok_new = posemb_new.shape[1]

    posemb_grid = posemb[0]
    

    gs_old_h = int(math.sqrt(len(posemb_grid)*hw_ratio))
    gs_old_w = gs_old_h // hw_ratio
    
    #print(len(posemb_grid), gs_old_w, gs_old_h)
    
    print('Resized position embedding from size:{} to size: {} with height:{} width: {}'.format(posemb.shape, posemb_new.shape, hight, width))
    posemb_grid = posemb_grid.reshape(1, gs_old_h, gs_old_w, -1).permute(0, 3, 1, 2)
    posemb_grid = F.interpolate(posemb_grid, size=(hight, width), mode='bilinear')
    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, hight * width, -1)
    posemb = posemb_grid
    return posemb



# gene matcher
def empty_gene_matcher(endpoints):
    gene_matcher = {
        'o2g': {},
        'g2o': {}
    }
    for k in endpoints.keys():
        gene_matcher['{}_2_g'.format(k)] = {}
    return gene_matcher

def find_all_indices(lst, label):
    return [index for index, value in enumerate(lst) if value == label]

def select_gene(o2g_matcher, m):
    gene_num = len(set(o2g_matcher))
    assert gene_num <= m <= len(o2g_matcher)
    A_count = Counter(o2g_matcher)
    assert gene_num == len(A_count) == (max(o2g_matcher)-min(o2g_matcher)+1)

    gene_count = {label:1 for label in range(min(o2g_matcher),max(o2g_matcher)+1)}
    to_gene = [i for i in range(min(o2g_matcher),max(o2g_matcher)+1)]

    while len(to_gene) < m:
        ratios = [gene_count[i]/A_count[i] for i in range(min(o2g_matcher),max(o2g_matcher)+1)]
        min_gene_index = ratios.index(min(ratios)) + min(o2g_matcher)
        to_gene.append(min_gene_index)
        gene_count[min_gene_index] += 1
    
    assert len(to_gene) == m

    return to_gene

def select_gene_qkv(o2g_matcher, m, num_heads):
    head_dim = len(o2g_matcher) // num_heads
    n = m // num_heads
    to_gene = []
    for head_idx in range(num_heads):
        head_matcher = o2g_matcher[head_idx*head_dim:(head_idx+1)*head_dim]
        to_gene.extend(select_gene(head_matcher,n))
    assert len(to_gene) == m
    return to_gene

def select_elements(o2g_matcher, m, c_start=0, o_start=0):
    # to_gene: for current layer
    gene_num = len(set(o2g_matcher))
    assert gene_num <= m <= len(o2g_matcher)
    A_count = Counter(o2g_matcher)
    assert gene_num == len(A_count) == (max(o2g_matcher)-min(o2g_matcher)+1)

    gene_count = {label:1 for label in range(min(o2g_matcher),max(o2g_matcher)+1)}
    to_gene = [i for i in range(min(o2g_matcher),max(o2g_matcher)+1)]

    while len(to_gene) < m:
        ratios = [gene_count[i]/A_count[i] for i in range(min(o2g_matcher),max(o2g_matcher)+1)]
        min_gene_index = ratios.index(min(ratios)) + min(o2g_matcher)
        to_gene.append(min_gene_index)
        gene_count[min_gene_index] += 1
    
    assert len(to_gene) == m

    # to_origin: for next layer
    to_origin = {}

    for label in range(min(o2g_matcher),max(o2g_matcher)+1):
        g2o = find_all_indices(o2g_matcher, label)
        g2o = [i+o_start for i in g2o]
        idxs = find_all_indices(to_gene, label)
        idxs = [idx+c_start for idx in idxs]
        chushu = len(g2o) // len(idxs)
        yushu = len(g2o) % len(idxs)
        pos = 0
        for idx in idxs:
            if yushu:
                to_origin[idx] = g2o[pos:pos+chushu+1]
                pos = pos+chushu+1
                yushu = yushu-1
            else:
                to_origin[idx] = g2o[pos:pos+chushu]
                pos = pos+chushu
        assert pos == len(g2o)
        assert yushu == 0
    assert len(to_gene) == len(to_origin)

    return to_gene, to_origin

def select_elements_qkv(o2g_matcher, m, num_heads):
    head_dim = len(o2g_matcher) // num_heads
    n = m // num_heads
    to_gene = []
    to_origin = {}
    for head_idx in range(num_heads):
        head_matcher = o2g_matcher[head_idx*head_dim:(head_idx+1)*head_dim]
        single_head_to_gene,single_head_to_origin = select_elements(head_matcher,n,head_idx*n,head_idx*head_dim)
        to_gene.extend(single_head_to_gene)
        to_origin.update(single_head_to_origin)

    assert len(to_gene) == len(to_origin)

    # q,k,v 都是一样的matcher
    gene_num = len(set(o2g_matcher))
    to_gene_qkv = []
    to_origin_qkv = {}
    for i in range(3):
        tmp_to_gene = [j+i*gene_num for j in to_gene]
        to_gene_qkv.extend(tmp_to_gene)
        tmp_to_origin = {k+i*m : [j+i*len(o2g_matcher) for j in v] for k,v in to_origin.items()}
        to_origin_qkv.update(tmp_to_origin)

    assert len(to_gene_qkv) == len(to_origin_qkv)

    # return to_gene, to_origin
    return to_gene, to_origin, to_gene_qkv, to_origin_qkv


# rowAverager & colAdder
def get_rowAveragers(o2g_matcher,c2g_matcher):
    # 构建端点模型与原模型的row_mapping
    def find_indices(lst, i):
        return [index for index, value in enumerate(lst) if value == i]
    gene_num = len(set(o2g_matcher))
    assert gene_num == len(set(c2g_matcher)) == max(o2g_matcher)+1 == max(c2g_matcher)+1
    row_mapping = {}
    for gene_idx in range(gene_num):
        o_idxs = find_indices(o2g_matcher,gene_idx)
        c_idxs = find_indices(c2g_matcher,gene_idx)
        shang = len(o_idxs) // len(c_idxs)
        yushu = len(o_idxs) % len(c_idxs)
        pos = 0
        for c_idx in c_idxs:
            row_mapping[c_idx] = o_idxs[pos:pos+shang+(yushu>0)]
            pos = pos+shang+1 if yushu>0 else pos+shang
            yushu -= 1
    # 构建row_mapping对应的权重矩阵
    new_col_count = len(c2g_matcher)
    original_col_count = len(o2g_matcher)
    weights = torch.zeros(new_col_count, original_col_count)
    # 填充权重矩阵
    for new_idx, original_idxs in row_mapping.items():
        for idx in original_idxs:
            weights[new_idx, idx] = 1.0/len(original_idxs)  # 将参与平均的行设置为1/len(original_idxs)

    return weights

def get_colAdders(o2g_matcher,c2g_matcher):
    # 构建端点模型与原模型的column_mapping
    def find_indices(lst, i):
        return [index for index, value in enumerate(lst) if value == i]
    gene_num = len(set(o2g_matcher))
    assert gene_num == len(set(c2g_matcher)) == max(o2g_matcher)+1 == max(c2g_matcher)+1
    column_mapping = {}
    for gene_idx in range(gene_num):
        o_idxs = find_indices(o2g_matcher,gene_idx)
        c_idxs = find_indices(c2g_matcher,gene_idx)
        shang = len(o_idxs) // len(c_idxs)
        yushu = len(o_idxs) % len(c_idxs)
        pos = 0
        for c_idx in c_idxs:
            column_mapping[c_idx] = o_idxs[pos:pos+shang+(yushu>0)]
            pos = pos+shang+1 if yushu>0 else pos+shang
            yushu -= 1
    # 构建column_mapping对应的权重矩阵
    new_col_count = len(c2g_matcher)
    original_col_count = len(o2g_matcher)
    weights = torch.zeros(new_col_count, original_col_count)
    # 填充权重矩阵
    for new_idx, original_idxs in column_mapping.items():
        for idx in original_idxs:
            weights[new_idx, idx] = 1.0  # 将参与加和的列设置为1

    return weights

def freeze_Adder_Averager(Adder_Averagers):
    for Adder_Averager in Adder_Averagers.values():
        if Adder_Averager is not None:
            for param in Adder_Averager.parameters():
                param.requires_grad = False

# param decoder
def Vector_Downsample(ori_vec,Averager=None):
    if Averager:
        return Averager(ori_vec)
    else:
        return ori_vec

def Matrix_Upsample(gene_weight,gene_matcher,colAdders=None):
    out_weight = gene_weight[gene_matcher]
    if colAdders:
        out_weight = colAdders(out_weight)
    return out_weight



class Attention_WeightChain(Attention):
    def __init__(
            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.,
            weight_chain_scale=2, colAdders=None, rowAveragers=None, endpoints={}
        ):
        super().__init__(dim=dim,num_heads=num_heads,qkv_bias=qkv_bias,qk_scale=qk_scale,attn_drop=attn_drop,proj_drop=proj_drop)

        self.head_dim = head_dim = dim // num_heads
        self.scales = {
            's1': (int(head_dim*weight_chain_scale))**-0.5,
            'entire': head_dim**-0.5
        }

        # qkv_bias居然是True!!居然是有bias的!!!

        self.weight_chain_scale = weight_chain_scale
        self.endpoints = endpoints
        # =====gene_matcher
        self.gene_matcher = empty_gene_matcher(endpoints)
        # =====不同尺寸模型
        self.qkvs = nn.ModuleDict({
            k: nn.Linear(int(dim*v), int(dim*3*v)) for k,v in endpoints.items()
        })
        self.projs = nn.ModuleDict({
            k: nn.Linear(int(dim*v), int(dim*v)) for k,v in endpoints.items()
        })
        # =====基因参数(训练 初始化为聚类中心)
        self.qkv_weight_gene = nn.Parameter(torch.randn(int(dim*3*weight_chain_scale), dim))
        self.qkv_bias_gene = nn.Parameter(torch.randn(dim*3))
        self.proj_weight_gene = nn.Parameter(torch.randn(int(dim*weight_chain_scale), dim))
        self.proj_bias_gene = nn.Parameter(torch.randn(dim))
        # =====列加和器 不训练
        self.qkv_colAdders = colAdders
        self.proj_colAdders = nn.ModuleDict({
            k: nn.Linear(dim, int(dim*v)) for k,v in endpoints.items()
        })
        self.proj_colAdders['entire'] = None
        # =====bias平均器 不训练 # qkv有bias!
        self.qkv_rowAveragers = nn.ModuleDict({
            k: nn.Linear(dim*3, int(dim*3*v)) for k,v in endpoints.items()
        })
        self.qkv_rowAveragers['entire'] = None
        self.proj_rowAveragers = rowAveragers

    def learner_init(self):
        scale_factors = [k for k in self.endpoints.keys() if k!='entire']
        param_names = ['q','k','v','qkv','proj']
        # 各端点模型的gene_matcher(矩阵每行对应的基因idx)
        for scale_factor in scale_factors:
            self.gene_matcher['{}_2_g'.format(scale_factor)]['q'] = select_gene_qkv(self.gene_matcher['o2g']['q'],self.qkvs['{}'.format(scale_factor)].weight.shape[0]//3,num_heads=self.num_heads)
            self.gene_matcher['{}_2_g'.format(scale_factor)]['k'] = select_gene_qkv(self.gene_matcher['o2g']['k'],self.qkvs['{}'.format(scale_factor)].weight.shape[0]//3,num_heads=self.num_heads)
            self.gene_matcher['{}_2_g'.format(scale_factor)]['v'] = select_gene_qkv(self.gene_matcher['o2g']['v'],self.qkvs['{}'.format(scale_factor)].weight.shape[0]//3,num_heads=self.num_heads)
            assert self.gene_matcher['{}_2_g'.format(scale_factor)]['q'] == self.gene_matcher['{}_2_g'.format(scale_factor)]['k'] == self.gene_matcher['{}_2_g'.format(scale_factor)]['v']
            qkv_gene_matcher = self.gene_matcher['{}_2_g'.format(scale_factor)]['q']
            gene_num = len(set(qkv_gene_matcher))
            self.gene_matcher['{}_2_g'.format(scale_factor)]['qkv'] = qkv_gene_matcher+[i+gene_num for i in qkv_gene_matcher]+[i+gene_num*2 for i in qkv_gene_matcher]
            self.gene_matcher['{}_2_g'.format(scale_factor)]['proj'] = select_gene(self.gene_matcher['o2g']['proj'],self.projs['{}'.format(scale_factor)].weight.shape[0])
        for param_name in param_names:
            self.gene_matcher['entire_2_g'][param_name] = self.gene_matcher['o2g'][param_name]
        # proj列加和器 # 所有attn.qkv和mlp.fc1的列加和器都一样
        freeze_Adder_Averager(self.proj_colAdders)
        for scale_factor in scale_factors:
            self.proj_colAdders['{}'.format(scale_factor)].weight.copy_(get_colAdders(self.gene_matcher['o2g']['v'],self.gene_matcher['{}_2_g'.format(scale_factor)]['v']))
        # bias平均器 # 所有attn.proj和mlp.fc2的bias平均器都一样 # qkv有bias!! 需要bias平均器
        freeze_Adder_Averager(self.qkv_rowAveragers)
        for scale_factor in scale_factors:
            self.qkv_rowAveragers['{}'.format(scale_factor)].weight.copy_(get_rowAveragers(self.gene_matcher['o2g']['qkv'],self.gene_matcher['{}_2_g'.format(scale_factor)]['qkv']))

    def param_detach(self,scale_factor=None):
        if scale_factor is not None:
            self.qkvs[scale_factor].weight.detach_()
            self.qkvs[scale_factor].bias.detach_()
            self.projs[scale_factor].weight.detach_()
            self.projs[scale_factor].bias.detach_()
        else:
            self.qkv.weight.detach_()
            self.qkv.bias.detach_()
            self.proj.weight.detach_()
            self.proj.bias.detach_()

    def param_copy(self,scale_factor=None):
        if scale_factor is not None:
            # 解码得到参数矩阵
            qkv_weight = Matrix_Upsample(
                self.qkv_weight_gene,self.gene_matcher['{}_2_g'.format(scale_factor)]['qkv'], self.qkv_colAdders[scale_factor]
            )
            proj_weight = Matrix_Upsample(
                self.proj_weight_gene,self.gene_matcher['{}_2_g'.format(scale_factor)]['proj'], self.proj_colAdders[scale_factor]
            )
            qkv_bias = Vector_Downsample(self.qkv_bias_gene,self.qkv_rowAveragers[scale_factor])
            proj_bias = Vector_Downsample(self.proj_bias_gene,self.proj_rowAveragers[scale_factor])
            # copy解码得到的参数矩阵
            self.qkvs[scale_factor].weight.copy_(qkv_weight)
            self.qkvs[scale_factor].bias.copy_(qkv_bias)
            self.projs[scale_factor].weight.copy_(proj_weight)
            self.projs[scale_factor].bias.copy_(proj_bias)
        else:
            self.qkv.weight.copy_(self.qkv_weight_gene[self.gene_matcher['o2g']['qkv']])
            self.qkv.bias.copy_(self.qkv_bias_gene)
            self.proj.weight.copy_(self.proj_weight_gene[self.gene_matcher['o2g']['proj']])
            self.proj.bias.copy_(self.proj_bias_gene)

    def forward(self, x, scale_factor=None):
        B, N, C = x.shape
        if scale_factor is not None:
            qkv = self.qkvs[scale_factor](x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        else:
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)

        if scale_factor is not None:
            attn = (q @ k.transpose(-2, -1)) * self.scales[scale_factor]
        else:
            attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        if scale_factor is not None:
            x = self.projs[scale_factor](x)
        else:
            x = self.proj(x)
        x = self.proj_drop(x)
        return x

    def forward_WeightChain(self, x, scale_factor=None):
        x = self.forward(x,scale_factor)
        return x


class Mlp_WeightChain(Mlp):
    def __init__(
            self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.,
            weight_chain_scale=2, colAdders=None, rowAveragers=None, endpoints={}
        ):
        super().__init__(in_features=in_features,hidden_features=hidden_features,out_features=out_features,act_layer=act_layer,drop=drop)

        out_features = out_features or in_features
        hidden_features = hidden_features or in_features

        self.weight_chain_scale = weight_chain_scale
        self.endpoints = endpoints
        # =====gene_matcher
        self.gene_matcher = empty_gene_matcher(endpoints)
        # =====不同尺寸模型
        self.fc1s = nn.ModuleDict({
            k: nn.Linear(int(in_features*v), int(hidden_features*v)) for k,v in endpoints.items()
        })
        self.fc2s = nn.ModuleDict({
            k: nn.Linear(int(hidden_features*v), int(out_features*v)) for k,v in endpoints.items()
        })
        # =====基因参数(训练 初始化为聚类中心)
        self.fc1_weight_gene = nn.Parameter(torch.randn(int(hidden_features*weight_chain_scale), in_features))
        self.fc1_bias_gene = nn.Parameter(torch.randn(hidden_features))
        self.fc2_weight_gene = nn.Parameter(torch.randn(int(out_features*weight_chain_scale), hidden_features))
        self.fc2_bias_gene = nn.Parameter(torch.randn(out_features))
        # =====列加和器
        self.fc1_colAdders = colAdders
        self.fc2_colAdders = nn.ModuleDict({
            k: nn.Linear(hidden_features, int(hidden_features*v)) for k,v in endpoints.items()
        })
        self.fc2_colAdders['entire'] = None
        # =====bias平均器
        self.fc1_rowAveragers = nn.ModuleDict({
            k: nn.Linear(hidden_features, int(hidden_features*v)) for k,v in endpoints.items()
        })
        self.fc1_rowAveragers['entire'] = None
        self.fc2_rowAveragers = rowAveragers

    def learner_init(self):
        scale_factors = [k for k in self.endpoints.keys() if k!='entire']
        # 每个端点模型的矩阵每行对应的基因idx
        for scale_factor in scale_factors:
            self.gene_matcher['{}_2_g'.format(scale_factor)]['fc1'] = select_gene(self.gene_matcher['o2g']['fc1'],self.fc1s['{}'.format(scale_factor)].weight.shape[0])
            self.gene_matcher['{}_2_g'.format(scale_factor)]['fc2'] = select_gene(self.gene_matcher['o2g']['fc2'],self.fc2s['{}'.format(scale_factor)].weight.shape[0])
        self.gene_matcher['entire_2_g']['fc1'] = self.gene_matcher['o2g']['fc1']
        self.gene_matcher['entire_2_g']['fc2'] = self.gene_matcher['o2g']['fc2']
        # fc2_列加和器 # 所有attn.qkv和mlp.fc1的列加和器都一样
        freeze_Adder_Averager(self.fc2_colAdders)
        for scale_factor in scale_factors:
            self.fc2_colAdders['{}'.format(scale_factor)].weight.copy_(get_colAdders(self.gene_matcher['o2g']['fc1'],self.gene_matcher['{}_2_g'.format(scale_factor)]['fc1']))
        # fc1_bias平均器 # 所有attn.proj和mlp.fc2的bias平均器都一样
        freeze_Adder_Averager(self.fc1_rowAveragers)
        for scale_factor in scale_factors:
            self.fc1_rowAveragers['{}'.format(scale_factor)].weight.copy_(get_rowAveragers(self.gene_matcher['o2g']['fc1'],self.gene_matcher['{}_2_g'.format(scale_factor)]['fc1']))

    def param_detach(self,scale_factor):
        if scale_factor is not None:
            self.fc1s[scale_factor].weight.detach_()
            self.fc1s[scale_factor].bias.detach_()
            self.fc2s[scale_factor].weight.detach_()
            self.fc2s[scale_factor].bias.detach_()
        else:
            self.fc1.weight.detach_()
            self.fc1.bias.detach_()
            self.fc2.weight.detach_()
            self.fc2.bias.detach_()

    def param_copy(self,scale_factor=None):
        if scale_factor is not None:
            # # 解码得到参数矩阵,加和/平均得到参数向量
            fc1_weight = Matrix_Upsample(
                self.fc1_weight_gene, self.gene_matcher['{}_2_g'.format(scale_factor)]['fc1'], self.fc1_colAdders[scale_factor]
            )
            fc2_weight = Matrix_Upsample(
                self.fc2_weight_gene, self.gene_matcher['{}_2_g'.format(scale_factor)]['fc2'], self.fc2_colAdders[scale_factor]
            )
            fc1_bias = Vector_Downsample(self.fc1_bias_gene,self.fc1_rowAveragers[scale_factor])
            fc2_bias = Vector_Downsample(self.fc2_bias_gene,self.fc2_rowAveragers[scale_factor])
            # # copy解码得到的参数矩阵
            self.fc1s[scale_factor].weight.copy_(fc1_weight)
            self.fc2s[scale_factor].weight.copy_(fc2_weight)
            self.fc1s[scale_factor].bias.copy_(fc1_bias)
            self.fc2s[scale_factor].bias.copy_(fc2_bias)
        else:
            self.fc1.weight.copy_(self.fc1_weight_gene[self.gene_matcher['o2g']['fc1']])
            self.fc2.weight.copy_(self.fc2_weight_gene[self.gene_matcher['o2g']['fc2']])
            self.fc1.bias.copy_(self.fc1_bias_gene)
            self.fc2.bias.copy_(self.fc2_bias_gene)

    def forward(self, x, scale_factor=None):
        if scale_factor is not None:
            x = self.fc1s[scale_factor](x)
        else:
            x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        if scale_factor is not None:
            x = self.fc2s[scale_factor](x)
        else:
            x = self.fc2(x)
        x = self.drop(x)
        return x

    def forward_WeightChain(self, x, scale_factor=None):
        x = self.forward(x,scale_factor)
        return x


class Block_WeightChain(Block):
    def __init__(
            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,
            weight_chain_scale=2, rowAveragers=None, endpoints={}
        ):
        super().__init__(dim=dim,num_heads=num_heads,mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,qk_scale=qk_scale,drop=drop,attn_drop=attn_drop,drop_path=drop_path,act_layer=act_layer,norm_layer=norm_layer)

        self.weight_chain_scale = weight_chain_scale
        self.endpoints = endpoints
        self.rowAveragers = rowAveragers

        self.norm1_gene = norm_layer(dim)
        self.norm2_gene = norm_layer(dim)
        self.norm1s = nn.ModuleDict({
            k: norm_layer(int(dim*v)) for k,v in endpoints.items()
        })
        self.norm2s = nn.ModuleDict({
            k: norm_layer(int(dim*v)) for k,v in endpoints.items()
        })

        self.attn = Attention_WeightChain(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,
            weight_chain_scale=weight_chain_scale, endpoints=endpoints
        )

        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp_WeightChain(
            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop,
            weight_chain_scale=weight_chain_scale, endpoints=endpoints
        )

    def param_detach(self,scale_factor=None):
        if scale_factor is not None:
            self.norm1s[scale_factor].weight.detach_()
            self.norm1s[scale_factor].bias.detach_()
            self.norm2s[scale_factor].weight.detach_()
            self.norm2s[scale_factor].bias.detach_()
        else:
            self.norm1.weight.detach_()
            self.norm1.bias.detach_()
            self.norm2.weight.detach_()
            self.norm2.bias.detach_() 

        self.attn.param_detach(scale_factor)
        self.mlp.param_detach(scale_factor)

    def param_copy(self,scale_factor=None):
        if scale_factor is not None:
            self.norm1s[scale_factor].weight.copy_(Vector_Downsample(self.norm1_gene.weight,self.rowAveragers[scale_factor]))
            self.norm1s[scale_factor].bias.copy_(Vector_Downsample(self.norm1_gene.bias,self.rowAveragers[scale_factor]))
            self.norm2s[scale_factor].weight.copy_(Vector_Downsample(self.norm2_gene.weight,self.rowAveragers[scale_factor]))
            self.norm2s[scale_factor].bias.copy_(Vector_Downsample(self.norm2_gene.bias,self.rowAveragers[scale_factor]))
        else:
            self.norm1.weight.copy_(self.norm1_gene.weight)
            self.norm1.bias.copy_(self.norm1_gene.bias)
            self.norm2.weight.copy_(self.norm2_gene.weight)
            self.norm2.bias.copy_(self.norm2_gene.bias)
        self.attn.param_copy(scale_factor)
        self.mlp.param_copy(scale_factor)

    def forward_WeightChain(self, x, scale_factor=None):
        x = x + self.drop_path(self.attn.forward_WeightChain(self.norm1s[scale_factor](x),scale_factor))
        x = x + self.drop_path(self.mlp.forward_WeightChain(self.norm2s[scale_factor](x),scale_factor))
        return x


class TransReID_WeightChain(TransReID):
    def __init__(
            self, img_size=224, patch_size=16, stride_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., camera=0, drop_path_rate=0., norm_layer=partial(nn.LayerNorm, eps=1e-6), local_feature=False, sie_xishu =1.0, hw_ratio=1, gem_pool=False, stem_conv=False,
            weight_chain_scale = 4, endpoints = {}
        ):
        super().__init__(img_size=img_size, patch_size=patch_size, stride_size=stride_size, in_chans=in_chans, num_classes=num_classes, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, camera=camera, drop_path_rate=drop_path_rate, norm_layer=norm_layer, local_feature=local_feature, sie_xishu=sie_xishu, hw_ratio=hw_ratio, gem_pool=gem_pool, stem_conv=stem_conv)

        self.weight_chain_scale = weight_chain_scale
        if not endpoints:
            endpoints = {
                's{}'.format(i): i for i in range(1,weight_chain_scale)
            }
            endpoints['entire'] = weight_chain_scale
        self.endpoints = endpoints

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        self.blocks = nn.ModuleList([
            Block_WeightChain(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,
                weight_chain_scale=self.weight_chain_scale, endpoints=endpoints)
            for i in range(depth)])

        # =====gene_matcher
        self.gene_matcher = empty_gene_matcher(endpoints)
        # =====不同尺寸模型
        self.dws = nn.ModuleDict({
            k: nn.Linear(embed_dim, int(embed_dim*v), bias=False) for k,v in endpoints.items()
        })
        self.norms = nn.ModuleDict({
            k: norm_layer(int(embed_dim*v)) for k,v in endpoints.items()
        })
        # =====基因参数(训练 初始化为聚类中心)
        self.dw_weight_gene = nn.Parameter(torch.randn(int(embed_dim*self.weight_chain_scale), embed_dim))
        self.norm_gene = norm_layer(embed_dim)
        # =====unity加和器/平均器
        self.colAdders = nn.ModuleDict({
            k: nn.Linear(embed_dim, int(embed_dim*v), bias=False) for k,v in endpoints.items()
        })
        self.colAdders['entire'] = None
        self.rowAveragers = nn.ModuleDict({
            k: nn.Linear(embed_dim, int(embed_dim*v), bias=False) for k,v in endpoints.items()
        })
        self.rowAveragers['entire'] = None

    def learner_init(self):
        scale_factors = [k for k in self.endpoints.keys() if k!='entire']
        print(scale_factors)
        # 各端点模型的gene_matcher(矩阵每行对应的基因idx)
        for scale_factor in scale_factors:
            self.gene_matcher['{}_2_g'.format(scale_factor)]['dw_proj_fc2'] = select_gene(self.gene_matcher['o2g']['dw_proj_fc2'],self.dws['{}'.format(scale_factor)].weight.shape[0])
        self.gene_matcher['entire_2_g']['dw_proj_fc2'] = self.gene_matcher['o2g']['dw_proj_fc2']
        # 列加和器 # 所有attn.qkv和mlp.fc1的列加和器都一样
        freeze_Adder_Averager(self.colAdders)
        for scale_factor in scale_factors:
            self.colAdders['{}'.format(scale_factor)].weight.copy_(get_colAdders(self.gene_matcher['o2g']['dw_proj_fc2'],self.gene_matcher['{}_2_g'.format(scale_factor)]['dw_proj_fc2']))
        # bias平均器 # 所有attn.proj和mlp.fc2的bias平均器都一样
        freeze_Adder_Averager(self.rowAveragers)
        for scale_factor in scale_factors:
            self.rowAveragers['{}'.format(scale_factor)].weight.copy_(get_rowAveragers(self.gene_matcher['o2g']['dw_proj_fc2'],self.gene_matcher['{}_2_g'.format(scale_factor)]['dw_proj_fc2']))

        for blk in self.blocks:
            blk.rowAveragers = self.rowAveragers
            blk.attn.qkv_colAdders = self.colAdders
            blk.attn.proj_rowAveragers = self.rowAveragers
            blk.attn.learner_init()
            blk.mlp.fc1_colAdders = self.colAdders
            blk.mlp.fc2_rowAveragers = self.rowAveragers
            blk.mlp.learner_init()

    def param_detach(self,scale_factor=None):
        if scale_factor is not None:
            self.dws[scale_factor].weight.detach_()
            self.norms[scale_factor].weight.detach_()
            self.norms[scale_factor].bias.detach_()
        else:
            self.dw.weight.detach_()
            self.norm.weight.detach_()
            self.norm.bias.detach_()

        for blk in self.blocks:
            blk.param_detach(scale_factor)

    def param_copy(self,scale_factor=None):
        if scale_factor is not None:
            # # 解码得到参数矩阵
            dw_weight = Matrix_Upsample(
                self.dw_weight_gene,self.gene_matcher['{}_2_g'.format(scale_factor)]['dw_proj_fc2'], colAdders=None # dw层不需要colAdders
            )
            norm_weight = Vector_Downsample(self.norm_gene.weight,self.rowAveragers[scale_factor])
            norm_bias = Vector_Downsample(self.norm_gene.bias,self.rowAveragers[scale_factor])
            # # copy解码得到的参数矩阵
            self.dws[scale_factor].weight.copy_(dw_weight)
            self.norms[scale_factor].weight.copy_(norm_weight)
            self.norms[scale_factor].bias.copy_(norm_bias)
        else:
            self.dw.weight.copy_(self.dw_weight_gene[self.gene_matcher['o2g']['dw_proj_fc2']])
            self.norm.weight.copy_(self.norm_gene.weight)
            self.norm.bias.copy_(self.norm_gene.bias)

        for blk in self.blocks:
            blk.param_copy(scale_factor)

    def forward_features(self, x, camera_id, scale_factor=None):
        B = x.shape[0]

        # with torch.no_grad():
        # =====patch_embed
        x = self.patch_embed(x)
        # =====cls_tokens, part_tokens
        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks
        part_tokens1 = self.part_token1.expand(B, -1, -1)
        part_tokens2 = self.part_token2.expand(B, -1, -1)
        part_tokens3 = self.part_token3.expand(B, -1, -1)
        x = torch.cat((cls_tokens, part_tokens1, part_tokens2, part_tokens3, x), dim=1)
        # =====pos_encoding
        x = x + torch.cat((self.cls_pos, self.part1_pos, self.part2_pos, self.part3_pos, self.pos_embed), dim=1)

        x = self.pos_drop(x)

        # =====dw
        if scale_factor is not None:
            x = self.dws[scale_factor](x)
        else:
            x = self.dw(x)

        # =====blocks
        if scale_factor is not None:
            for blk in self.blocks:
                x = blk.forward_WeightChain(x, scale_factor=scale_factor)
        else:
            for blk in self.blocks:
                x = blk(x)

        # =====norm
        if scale_factor is not None:
            x = self.norms[scale_factor](x)
        else:
            x = self.norm(x)

        if self.gem_pool:
            gf = self.gem(x[:,1:].permute(0,2,1)).squeeze()
            return x[:, 0] + gf

        return x[:, 0], x[:, 1], x[:, 2], x[:, 3]

    def forward(self, x, cam_label=None, scale_factor=None):
        global_feat, local_feat_1, local_feat_2, local_feat_3 = self.forward_features(x, cam_label, scale_factor=scale_factor)
        return global_feat, local_feat_1, local_feat_2, local_feat_3

    def forward_WeightChain(self, x, cam_label=None, scale_factor=None):
        if scale_factor is not None:
            self.param_detach(scale_factor)
            self.param_copy(scale_factor)
        x = self.forward(x,cam_label,scale_factor)
        return x

    def get_refining_loss(self):
        # dw
        res = self.dw.weight - self.dw_weight_gene[self.gene_matcher['o2g']['dw_proj_fc2']]
        loss = (res*res).sum()
        # norm
        res = self.norm.weight - self.norm_gene.weight
        loss += (res*res).sum()
        res = self.norm.bias - self.norm_gene.bias
        loss += (res*res).sum()

        for block in self.blocks:
            # weight
            res = block.attn.qkv.weight - block.attn.qkv_weight_gene[block.attn.gene_matcher['o2g']['qkv']]
            loss += (res*res).sum()
            res = block.attn.proj.weight - block.attn.proj_weight_gene[block.attn.gene_matcher['o2g']['proj']]
            loss += (res*res).sum()
            res = block.mlp.fc1.weight - block.mlp.fc1_weight_gene[block.mlp.gene_matcher['o2g']['fc1']]
            loss += (res*res).sum()
            res = block.mlp.fc2.weight - block.mlp.fc2_weight_gene[block.mlp.gene_matcher['o2g']['fc2']]
            loss += (res*res).sum()
            # bias
            res = block.attn.qkv.bias - block.attn.qkv_bias_gene
            loss += (res*res).sum()
            res = block.attn.proj.bias - block.attn.proj_bias_gene
            loss += (res*res).sum()
            res = block.mlp.fc1.bias - block.mlp.fc1_bias_gene
            loss += (res*res).sum()
            res = block.mlp.fc2.bias - block.mlp.fc2_bias_gene
            loss += (res*res).sum()
            # norm
            res = block.norm1.weight - block.norm1_gene.weight
            loss += (res*res).sum()
            res = block.norm1.bias - block.norm1_gene.bias
            loss += (res*res).sum()
            res = block.norm2.weight - block.norm2_gene.weight
            loss += (res*res).sum()
            res = block.norm2.bias - block.norm2_gene.bias
            loss += (res*res).sum()

        return loss

    def Cluster_Center_Init(self):
        print("Initialize Weight Chain with Cluster Centers...")
        # norm
        self.state_dict()['norm_gene.weight'].copy_(self.state_dict()['norm.weight'])
        self.state_dict()['norm_gene.bias'].copy_(self.state_dict()['norm.bias'])
        # dw
        for k,v in self.gene_matcher['g2o']['dw_proj_fc2'].items():
            self.state_dict()['dw_weight_gene'][k].copy_(self.state_dict()['dw.weight'][v].mean(dim=0))

        for block in self.blocks:
            # attn.qkv
            for k,v in block.attn.gene_matcher['g2o']['qkv'].items():
                block.state_dict()['attn.qkv_weight_gene'][k].copy_(block.state_dict()['attn.qkv.weight'][v].mean(dim=0))
            block.state_dict()['attn.qkv_bias_gene'].copy_(block.state_dict()['attn.qkv.bias'])
            # attn.proj
            for k,v in self.gene_matcher['g2o']['dw_proj_fc2'].items():
                block.state_dict()['attn.proj_weight_gene'][k].copy_(block.state_dict()['attn.proj.weight'][v].mean(dim=0))
            block.state_dict()['attn.proj_bias_gene'].copy_(block.state_dict()['attn.proj.bias'])
            # mlp.fc1
            for k,v in block.mlp.gene_matcher['g2o']['fc1'].items():
                block.state_dict()['mlp.fc1_weight_gene'][k].copy_(block.state_dict()['mlp.fc1.weight'][v].mean(dim=0))
            block.state_dict()['mlp.fc1_bias_gene'].copy_(block.state_dict()['mlp.fc1.bias'])
            # mlp.fc2
            for k,v in self.gene_matcher['g2o']['dw_proj_fc2'].items():
                block.state_dict()['mlp.fc2_weight_gene'][k].copy_(block.state_dict()['mlp.fc2.weight'][v].mean(dim=0))
            block.state_dict()['mlp.fc2_bias_gene'].copy_(block.state_dict()['mlp.fc2.bias'])
            # norm1
            block.state_dict()['norm1_gene.weight'].copy_(block.state_dict()['norm1.weight'])
            block.state_dict()['norm1_gene.bias'].copy_(block.state_dict()['norm1.bias'])
            # norm2
            block.state_dict()['norm2_gene.weight'].copy_(block.state_dict()['norm2.weight'])
            block.state_dict()['norm2_gene.bias'].copy_(block.state_dict()['norm2.bias'])

    def Pretrained_Weight_Clustering(self):
        joint_weights_normalized = normalize(np.array(self.dw.weight.data))
        for block_idx, block in enumerate(self.blocks):
            print("Clustering block "+str(block_idx))
            # ======================MSA qkv======================
            qkv = block.attn.qkv.weight.data.permute(1, 0).reshape(self.embed_dim, 3, self.embed_dim).permute(1, 2, 0)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            q_weights_normalized = normalize(np.array(q.view(q.shape[0],-1)))
            k_weights_normalized = normalize(np.array(k.view(k.shape[0],-1)))
            v_weights_normalized = normalize(np.array(v.view(v.shape[0],-1)))
            qkv_weights_normalized = np.hstack((q_weights_normalized,k_weights_normalized,v_weights_normalized))
            n_clusters = int(block.attn.head_dim * self.weight_chain_scale)
            cluster = AgglomerativeClustering(n_clusters=n_clusters)

            qkv_labels = []
            for head_idx in range(block.attn.num_heads):
                head_qkv_weights_normalized = qkv_weights_normalized[head_idx*block.attn.head_dim:(head_idx+1)*block.attn.head_dim]
                head_qkv_labels = cluster.fit_predict(head_qkv_weights_normalized).tolist()
                assert n_clusters == len(set(head_qkv_labels))
                head_qkv_labels = [head_idx*n_clusters+i for i in head_qkv_labels]
                qkv_labels.extend(head_qkv_labels)

            block.attn.gene_matcher['o2g']['q'] = qkv_labels
            block.attn.gene_matcher['o2g']['k'] = qkv_labels
            block.attn.gene_matcher['o2g']['v'] = qkv_labels
            block.attn.gene_matcher['o2g']['qkv'] = qkv_labels + [i+int(self.embed_dim*self.weight_chain_scale) for i in qkv_labels] + [i+2*int(self.embed_dim*self.weight_chain_scale) for i in qkv_labels]
            block.attn.gene_matcher['g2o']['qkv'] = {label: find_all_indices(block.attn.gene_matcher['o2g']['qkv'], label) for label in range(int(self.embed_dim*3*self.weight_chain_scale))}

            # ======================MLP fc1======================
            fc1_weights_normalized = normalize(np.array(block.mlp.fc1.weight.data))
            n_clusters = int(fc1_weights_normalized.shape[0] * self.weight_chain_scale)
            cluster = AgglomerativeClustering(n_clusters=n_clusters)

            labels = cluster.fit_predict(fc1_weights_normalized).tolist()
            assert len(set(labels)) == n_clusters
            block.mlp.gene_matcher['o2g']['fc1'] = labels
            block.mlp.gene_matcher['g2o']['fc1'] = {label: find_all_indices(labels, label) for label in range(n_clusters)}

            # ======================MSA proj======================
            proj_weights_normalized = normalize(np.array(block.attn.proj.weight.data))
            joint_weights_normalized = np.hstack((joint_weights_normalized,proj_weights_normalized))

            # ======================MLP fc2======================
            fc2_weights_normalized = normalize(np.array(block.mlp.fc2.weight.data))
            joint_weights_normalized = np.hstack((joint_weights_normalized,fc2_weights_normalized))

        n_clusters = int(self.embed_dim * self.weight_chain_scale)
        cluster = AgglomerativeClustering(n_clusters=n_clusters)

        labels = cluster.fit_predict(joint_weights_normalized).tolist()
        assert len(set(labels)) == n_clusters
        self.gene_matcher['o2g']['dw_proj_fc2'] = labels
        self.gene_matcher['g2o']['dw_proj_fc2'] = {label: find_all_indices(labels, label) for label in range(n_clusters)}
        for block_idx, block in enumerate(self.blocks):
            block.attn.gene_matcher['o2g']['proj'] = labels
            block.attn.gene_matcher['g2o']['proj'] = {label: find_all_indices(labels, label) for label in range(n_clusters)}
            block.mlp.gene_matcher['o2g']['fc2'] = labels
            block.mlp.gene_matcher['g2o']['fc2'] = {label: find_all_indices(labels, label) for label in range(n_clusters)}

    def save_gene_matcher(self,save_path):
        summary_dict = empty_gene_matcher(self.endpoints)
        attn_keys = ['q','k','v','qkv','proj']
        mlp_keys = ['fc1','fc2']

        for k in summary_dict.keys():
            if 'dw_proj_fc2' in self.gene_matcher[k]:
                summary_dict[k]['dw_proj_fc2'] = self.gene_matcher[k]['dw_proj_fc2']

            for block_idx, block in enumerate(self.blocks):
                for ak in attn_keys:
                    if ak in block.attn.gene_matcher[k]:
                        summary_dict[k]['blocks.{}.attn.{}'.format(block_idx,ak)] = block.attn.gene_matcher[k][ak]
                for mk in mlp_keys:
                    if mk in block.mlp.gene_matcher[k]:
                        summary_dict[k]['blocks.{}.mlp.{}'.format(block_idx,mk)] = block.mlp.gene_matcher[k][mk]

        json.dump(summary_dict, open(save_path,'w'))

    def load_gene_matcher(self,load_path):
        print("Loading gene matcher...")
        summary_dict = json.load(open(load_path))
        attn_keys = ['q','k','v','qkv','proj']
        mlp_keys = ['fc1','fc2']

        for k in summary_dict.keys():
            if 'dw_proj_fc2' in summary_dict[k]:
                self.gene_matcher[k]['dw_proj_fc2'] = summary_dict[k]['dw_proj_fc2']
                if k == 'g2o':
                    self.gene_matcher[k]['dw_proj_fc2'] = {int(k):v for k,v in summary_dict[k]['dw_proj_fc2'].items()}

            for block_idx, block in enumerate(self.blocks):
                for ak in attn_keys:
                    if 'blocks.{}.attn.{}'.format(block_idx,ak) in summary_dict[k]:
                        block.attn.gene_matcher[k][ak] = summary_dict[k]['blocks.{}.attn.{}'.format(block_idx,ak)]
                        if k == 'g2o':
                            block.attn.gene_matcher[k][ak] = {int(k):v for k,v in summary_dict[k]['blocks.{}.attn.{}'.format(block_idx,ak)].items()}
                for mk in mlp_keys:
                    if 'blocks.{}.mlp.{}'.format(block_idx,mk) in summary_dict[k]:
                        block.mlp.gene_matcher[k][mk] = summary_dict[k]['blocks.{}.mlp.{}'.format(block_idx,mk)]
                        if k == 'g2o':
                            block.mlp.gene_matcher[k][mk] = {int(k):v for k,v in summary_dict[k]['blocks.{}.mlp.{}'.format(block_idx,mk)].items()}

    def get_decoded_base(self,cfg,camera_num,backbone_embed_dim,scale_factor='entire'):
        __factory_T_type = {
            'vit_base_patch16_224_TransReID': ori_vit_base_patch16_224_TransReID,
            'deit_base_patch16_224_TransReID': ori_vit_base_patch16_224_TransReID,
            'vit_small_patch16_224_TransReID': ori_vit_small_patch16_224_TransReID,
            'deit_small_patch16_224_TransReID': ori_vit_small_patch16_224_TransReID
        }
        base = __factory_T_type[cfg.MODEL.TRANSFORMER_TYPE](
            img_size=cfg.INPUT.SIZE_TRAIN, sie_xishu=cfg.MODEL.SIE_COE, camera=camera_num, stride_size=cfg.MODEL.STRIDE_SIZE, drop_path_rate=cfg.MODEL.DROP_PATH, drop_rate= cfg.MODEL.DROP_OUT,attn_drop_rate=cfg.MODEL.ATT_DROP_RATE, gem_pool=cfg.MODEL.GEM_POOLING, stem_conv=cfg.MODEL.STEM_CONV,
            backbone_embed_dim=backbone_embed_dim
        )

        # # ==================dw之前的部分模块直接替换==================
        for para_name, para_val in self.patch_embed.named_parameters():
            base.patch_embed.state_dict()[para_name].copy_(para_val)
        for para_name in ['cls_token','part_token1','part_token2','part_token3','cls_pos','part1_pos','part2_pos','part3_pos','pos_embed']:
            base.state_dict()[para_name].copy_(self.state_dict()[para_name])

        # # ==================dw及之后的部分参数重新赋值==================
        # 根据backbone_embed_dim构建lc2o,c2g,c2o
        self.gene_matcher['dw_proj_fc2_2g'],self.gene_matcher['dw_proj_fc2_2o'] = select_elements(self.gene_matcher['o2g']['dw_proj_fc2'],backbone_embed_dim)
        for block in self.blocks:
            # attn.qkv
            block.attn.gene_matcher['q_2g'],block.attn.gene_matcher['q_2o'],block.attn.gene_matcher['qkv_2g'],block.attn.gene_matcher['qkv_2o'] = select_elements_qkv(block.attn.gene_matcher['o2g']['q'],backbone_embed_dim,self.num_heads)
            # mlp.fc1
            block.mlp.gene_matcher['fc1_2g'],block.mlp.gene_matcher['fc1_2o'] = select_elements(block.mlp.gene_matcher['o2g']['fc1'],backbone_embed_dim*4)

        # 根据lc2o,c2g,c2o赋值
        # dw
        c2g = self.gene_matcher['dw_proj_fc2_2g']
        base.state_dict()['dw.weight'].copy_(self.state_dict()['dw_weight_gene'][c2g])
        for decoded_block,block in zip(base.blocks,self.blocks):
            # attn.qkv
            lc2o = self.gene_matcher['dw_proj_fc2_2o']
            c2g = block.attn.gene_matcher['qkv_2g']
            c2o = block.attn.gene_matcher['qkv_2o']
            for k,v in lc2o.items():
                decoded_block.state_dict()['attn.qkv.weight'][:,k].copy_(block.state_dict()['attn.qkv_weight_gene'][c2g][:,v].sum(dim=1))
            for k,v in c2o.items():
                decoded_block.state_dict()['attn.qkv.bias'][k].copy_(block.state_dict()['attn.qkv_bias_gene'][v].mean(dim=0))
            # attn.proj
            lc2o = block.attn.gene_matcher['q_2o']
            c2g = self.gene_matcher['dw_proj_fc2_2g']
            c2o = self.gene_matcher['dw_proj_fc2_2o']
            for k,v in lc2o.items():
                decoded_block.state_dict()['attn.proj.weight'][:,k].copy_(block.state_dict()['attn.proj_weight_gene'][c2g][:,v].sum(dim=1))
            block.state_dict()['attn.proj_bias_gene'].copy_(block.state_dict()['attn.proj.bias'])
            for k,v in c2o.items():
                decoded_block.state_dict()['attn.proj.bias'][k].copy_(block.state_dict()['attn.proj_bias_gene'][v].mean(dim=0))
            # mlp.fc1
            lc2o = self.gene_matcher['dw_proj_fc2_2o']
            c2g = block.mlp.gene_matcher['fc1_2g']
            c2o = block.mlp.gene_matcher['fc1_2o']
            for k,v in lc2o.items():
                decoded_block.state_dict()['mlp.fc1.weight'][:,k].copy_(block.state_dict()['mlp.fc1_weight_gene'][c2g][:,v].sum(dim=1))
            for k,v in c2o.items():
                decoded_block.state_dict()['mlp.fc1.bias'][k].copy_(block.state_dict()['mlp.fc1_bias_gene'][v].mean(dim=0))
            # mlp.fc2
            lc2o = block.mlp.gene_matcher['fc1_2o']
            c2g = self.gene_matcher['dw_proj_fc2_2g']
            c2o = self.gene_matcher['dw_proj_fc2_2o']
            for k,v in lc2o.items():
                decoded_block.state_dict()['mlp.fc2.weight'][:,k].copy_(block.state_dict()['mlp.fc2_weight_gene'][c2g][:,v].sum(dim=1))
            for k,v in c2o.items():
                decoded_block.state_dict()['mlp.fc2.bias'][k].copy_(block.state_dict()['mlp.fc2_bias_gene'][v].mean(dim=0))
            # norm1
            c2o = self.gene_matcher['dw_proj_fc2_2o']
            for k,v in c2o.items():
                decoded_block.state_dict()['norm1.weight'][k].copy_(block.state_dict()['norm1_gene.weight'][v].mean(dim=0))
                decoded_block.state_dict()['norm1.bias'][k].copy_(block.state_dict()['norm1_gene.bias'][v].mean(dim=0))
            # norm2
            c2o = self.gene_matcher['dw_proj_fc2_2o']
            for k,v in c2o.items():
                decoded_block.state_dict()['norm2.weight'][k].copy_(block.state_dict()['norm2_gene.weight'][v].mean(dim=0))
                decoded_block.state_dict()['norm2.bias'][k].copy_(block.state_dict()['norm2_gene.bias'][v].mean(dim=0))
        # norm
        c2o = self.gene_matcher['dw_proj_fc2_2o']
        for k,v in c2o.items():
            base.state_dict()['norm.weight'][k].copy_(self.state_dict()['norm_gene.weight'][v].mean(dim=0))
            base.state_dict()['norm.bias'][k].copy_(self.state_dict()['norm_gene.bias'][v].mean(dim=0))

        return base



def vit_base_patch16_224_TransReID(
        img_size=(256, 128), stride_size=16, drop_path_rate=0.1, camera=0, local_feature=False,sie_xishu=1.5,
        weight_chain_scale=2, **kwargs
    ):
    model = TransReID_WeightChain(
        img_size=img_size, patch_size=16, stride_size=stride_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True, camera=camera, drop_path_rate=drop_path_rate, sie_xishu=sie_xishu, local_feature=local_feature,
        weight_chain_scale=weight_chain_scale, **kwargs
        )
    return model

def vit_small_patch16_224_TransReID(
        img_size=(256, 128), stride_size=16, drop_path_rate=0.1, camera=0, local_feature=False, sie_xishu=1.5,
        weight_chain_scale=2, **kwargs
    ):
    model = TransReID_WeightChain(
        img_size=img_size, patch_size=16, stride_size=stride_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,drop_path_rate=drop_path_rate, camera=camera, sie_xishu=sie_xishu, local_feature=local_feature, 
        weight_chain_scale=weight_chain_scale, **kwargs
        )
    # model = TransReID(img_size=img_size, patch_size=16, stride_size=stride_size, embed_dim=384, depth=12, num_heads=8, mlp_ratio=4, qkv_bias=True,drop_path_rate=drop_path_rate, camera=camera, sie_xishu=sie_xishu, local_feature=local_feature,  **kwargs)
    model.in_planes = 384
    return model


def decoded_vit_base_patch16_224_TransReID(
        img_size=(256, 128), stride_size=16, drop_path_rate=0.1, camera=0, local_feature=False,sie_xishu=1.5,
        weight_chain_scale=2, embed_dim=768, **kwargs
    ):
    num_heads = 12
    assert (embed_dim % num_heads == 0)
    model = TransReID_WeightChain(
        img_size=img_size, patch_size=16, stride_size=stride_size, embed_dim=embed_dim, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True, camera=camera, drop_path_rate=drop_path_rate, sie_xishu=sie_xishu, local_feature=local_feature,
        weight_chain_scale=weight_chain_scale, **kwargs
        )
    return model

def decoded_vit_small_patch16_224_TransReID(
        img_size=(256, 128), stride_size=16, drop_path_rate=0.1, camera=0, local_feature=False, sie_xishu=1.5,
        weight_chain_scale=2, depth=12, embed_dim=384, **kwargs
    ):
    num_heads = 6
    assert (embed_dim % num_heads == 0)
    model = TransReID_WeightChain(
        img_size=img_size, patch_size=16, stride_size=stride_size, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=4, qkv_bias=True,drop_path_rate=drop_path_rate, camera=camera, sie_xishu=sie_xishu, local_feature=local_feature, 
        weight_chain_scale=weight_chain_scale, **kwargs
        )
    # model = TransReID(img_size=img_size, patch_size=16, stride_size=stride_size, embed_dim=384, depth=12, num_heads=8, mlp_ratio=4, qkv_bias=True,drop_path_rate=drop_path_rate, camera=camera, sie_xishu=sie_xishu, local_feature=local_feature,  **kwargs)
    model.in_planes = 384
    return model